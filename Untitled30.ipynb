{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17019e52-bbbb-42f7-9b69-b35a4e692f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded. Shape: (38765, 3). Columns: ['Member_number', 'Date', 'itemDescription']\n",
      "Guessed transaction column: Date, item column: itemDescription\n",
      "Preprocessing transactions and building basket matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_14236\\2289578516.py:97: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  first_dtype = basket_df.dtypes[0]\n",
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_14236\\2289578516.py:98: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if pd.api.types.is_sparse(first_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built basket matrix. Transactions: 14963, Unique items: 167\n",
      "Saved HDF5 via h5py: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\processed_data.h5\n",
      "[mining] Attempt with min_support=0.0100\n",
      "[prune] Transactions: 14963, Items after prune: 64\n",
      "Mining finished (used_min_support=0.0100). Found 69 frequent_itemsets and 0 rules.\n",
      "Saved PKL: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\artifacts.pkl\n",
      "Saved YAML: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\metadata.yaml\n",
      "Saved JSON: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\association_rules.json\n",
      "All artifacts generated successfully.\n",
      "Files written to: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\n",
      "No rules generated with the chosen thresholds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "generate_artifacts_with_serial_rules.py\n",
    "\n",
    "Memory-aware market-basket artifact generator.\n",
    "Saves serializable rules (rules_serial) and provides a top-level factory to recreate recommend().\n",
    "\n",
    "Paths are hard-coded per user's request; adjust if needed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "# ---------- USER PATHS ----------\n",
    "INPUT_CSV = r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\archive (1)\\Groceries_dataset.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\"\n",
    "# -------------------------------\n",
    "\n",
    "# Mining config (tweak if needed)\n",
    "INITIAL_MIN_SUPPORT = 0.01\n",
    "MIN_CONFIDENCE = 0.20\n",
    "MIN_SUPPORT_STEP = 0.01\n",
    "MAX_RETRIES = 4\n",
    "MAX_ITEMSET_LEN = None  # set to 3 to limit itemset sizes if memory/time is an issue\n",
    "\n",
    "# ------------------ Utility & Processing ------------------\n",
    "\n",
    "def ensure_output_dir(path: str) -> Path:\n",
    "    p = Path(path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def load_dataset(csv_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Input CSV not found: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "    return df\n",
    "\n",
    "def guess_columns(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    object_cols = [c for c, dt in df.dtypes.items() if pd.api.types.is_object_dtype(dt) or pd.api.types.is_string_dtype(dt)]\n",
    "    tx_candidates, item_candidates = [], []\n",
    "    tx_names = [\"invoice\", \"invoiceno\", \"transaction\", \"orderid\", \"order_id\", \"date\", \"receipt\", \"id\"]\n",
    "    item_names = [\"item\", \"itemdescription\", \"product\", \"description\", \"item_name\"]\n",
    "    for col in object_cols:\n",
    "        cl = col.lower()\n",
    "        if any(name in cl for name in tx_names):\n",
    "            tx_candidates.append(col)\n",
    "        if any(name in cl for name in item_names):\n",
    "            item_candidates.append(col)\n",
    "    tx_col = tx_candidates[0] if tx_candidates else (object_cols[0] if object_cols else None)\n",
    "    item_col = item_candidates[0] if item_candidates else (object_cols[1] if len(object_cols) >= 2 else (object_cols[0] if object_cols else None))\n",
    "    return tx_col, item_col\n",
    "\n",
    "def preprocess_transactions(df: pd.DataFrame, tx_col: str, item_col: str):\n",
    "    df = df.copy()\n",
    "    df[item_col] = df[item_col].astype(str).str.strip()\n",
    "    df = df[df[item_col].notna() & (df[item_col] != '')]\n",
    "\n",
    "    if tx_col is None:\n",
    "        df['_TX_AUTO_'] = range(len(df))\n",
    "        tx_col = '_TX_AUTO_'\n",
    "\n",
    "    # If Date was chosen and Member_number present, group by member+date (session-level)\n",
    "    if tx_col.lower() == 'date' and 'Member_number' in df.columns:\n",
    "        grouped = df.groupby(['Member_number', 'Date'])[item_col].apply(lambda s: list(pd.Series(s).astype(str).str.strip().unique()))\n",
    "    else:\n",
    "        grouped = df.groupby(tx_col)[item_col].apply(lambda s: list(pd.Series(s).astype(str).str.strip().unique()))\n",
    "\n",
    "    transactions = grouped.tolist()\n",
    "    tx_index = grouped.index.tolist()\n",
    "\n",
    "    # Try sparse MultiLabelBinarizer if supported\n",
    "    try:\n",
    "        mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "        basket_sparse = mlb.fit_transform(transactions)\n",
    "        basket_df = pd.DataFrame.sparse.from_spmatrix(basket_sparse, index=tx_index, columns=mlb.classes_)\n",
    "    except TypeError:\n",
    "        mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "        basket = mlb.fit_transform(transactions)\n",
    "        basket_df = pd.DataFrame(basket, index=tx_index, columns=mlb.classes_)\n",
    "\n",
    "    # Make sure dtype is numeric 0/1 (use int8 to save memory)\n",
    "    try:\n",
    "        first_dtype = basket_df.dtypes[0]\n",
    "        if pd.api.types.is_sparse(first_dtype):\n",
    "            basket_df = basket_df.astype('int8')\n",
    "        else:\n",
    "            basket_df = basket_df.astype('int8')\n",
    "    except Exception:\n",
    "        basket_df = basket_df.astype('int8')\n",
    "\n",
    "    return transactions, basket_df, mlb\n",
    "\n",
    "def prune_items_by_support(basket_df: pd.DataFrame, min_support: float):\n",
    "    n_tx = basket_df.shape[0]\n",
    "    item_counts = basket_df.sum(axis=0)\n",
    "    item_supports = item_counts / float(n_tx)\n",
    "    keep_items = item_supports[item_supports >= min_support].index.tolist()\n",
    "    pruned_df = basket_df.loc[:, keep_items].copy()\n",
    "    return pruned_df, item_supports\n",
    "\n",
    "def run_mining_with_fallback(basket_df: pd.DataFrame, min_support=0.01, min_confidence=0.2, max_len=None):\n",
    "    cur_support = min_support\n",
    "    retries = 0\n",
    "    while True:\n",
    "        print(f\"[mining] Attempt with min_support={cur_support:.4f}\")\n",
    "        try:\n",
    "            pruned_df, item_supports = prune_items_by_support(basket_df, cur_support)\n",
    "            print(f\"[prune] Transactions: {pruned_df.shape[0]}, Items after prune: {pruned_df.shape[1]}\")\n",
    "            if pruned_df.shape[1] == 0:\n",
    "                raise ValueError(\"No items left after pruning. Raise dataset quality or lower min_support.\")\n",
    "\n",
    "            bool_df = pruned_df.astype(bool)\n",
    "            frequent_itemsets = fpgrowth(bool_df, min_support=cur_support, use_colnames=True, max_len=max_len)\n",
    "\n",
    "            if frequent_itemsets.empty:\n",
    "                rules = pd.DataFrame()\n",
    "            else:\n",
    "                frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "                rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "                if not rules.empty:\n",
    "                    rules = rules.sort_values(['lift', 'confidence', 'support'], ascending=[False, False, False])\n",
    "\n",
    "            return frequent_itemsets, rules, cur_support\n",
    "\n",
    "        except MemoryError as me:\n",
    "            retries += 1\n",
    "            warnings.warn(f\"MemoryError during mining attempt {retries}: {me}. Increasing min_support and retrying.\")\n",
    "            cur_support += MIN_SUPPORT_STEP\n",
    "            if retries > MAX_RETRIES:\n",
    "                raise MemoryError(f\"Mining failed after {retries} retries.\") from me\n",
    "\n",
    "# ------------------ Saving processed data (robust) ------------------\n",
    "\n",
    "def save_processed_data_array(basket_df: pd.DataFrame, out_base: Path):\n",
    "    \"\"\"\n",
    "    Save processed basket as HDF5 via h5py if present; otherwise as compressed NPZ.\n",
    "    Stores:\n",
    "      - data: 2D uint8 array (transactions x items)\n",
    "      - index: array of strings\n",
    "      - columns: array of strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_np = basket_df.to_numpy(dtype=np.uint8)\n",
    "    except Exception:\n",
    "        data_np = np.array(basket_df.values, dtype=np.uint8)\n",
    "\n",
    "    index_arr = np.array([str(x) for x in basket_df.index], dtype=object)\n",
    "    cols_arr = np.array([str(x) for x in basket_df.columns], dtype=object)\n",
    "\n",
    "    # Try h5py first\n",
    "    try:\n",
    "        import h5py\n",
    "        h5_path = out_base.with_suffix('.h5')\n",
    "        with h5py.File(str(h5_path), 'w') as hf:\n",
    "            hf.create_dataset('data', data=data_np, compression='gzip', compression_opts=4)\n",
    "            dt = h5py.string_dtype(encoding='utf-8')\n",
    "            hf.create_dataset('index', data=index_arr.astype('S'), dtype=dt)\n",
    "            hf.create_dataset('columns', data=cols_arr.astype('S'), dtype=dt)\n",
    "            hf.attrs['saved_on'] = datetime.datetime.now().isoformat()\n",
    "        print(f\"Saved HDF5 via h5py: {h5_path}\")\n",
    "        return h5_path\n",
    "    except Exception:\n",
    "        npz_path = out_base.with_suffix('.npz')\n",
    "        np.savez_compressed(str(npz_path), data=data_np, index=index_arr, columns=cols_arr)\n",
    "        print(f\"h5py not available or writing failed — saved compressed NPZ: {npz_path}\")\n",
    "        return npz_path\n",
    "\n",
    "def save_pickle(obj, out_path: str):\n",
    "    joblib.dump(obj, out_path)\n",
    "    print(f\"Saved PKL: {out_path}\")\n",
    "\n",
    "def save_yaml(data: dict, out_path: str):\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.safe_dump(data, f, sort_keys=False)\n",
    "    print(f\"Saved YAML: {out_path}\")\n",
    "\n",
    "def save_json(data, out_path: str):\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, default=str)\n",
    "    print(f\"Saved JSON: {out_path}\")\n",
    "\n",
    "# ------------------ Rules serialization & recommender factory ------------------\n",
    "\n",
    "def rules_to_serializable(rules_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    if rules_df is None or rules_df.empty:\n",
    "        return []\n",
    "    serial = []\n",
    "    for _, row in rules_df.iterrows():\n",
    "        serial.append({\n",
    "            \"antecedents\": sorted(list(row['antecedents'])) if hasattr(row['antecedents'], '__iter__') else [row['antecedents']],\n",
    "            \"consequents\": sorted(list(row['consequents'])) if hasattr(row['consequents'], '__iter__') else [row['consequents']],\n",
    "            \"support\": float(row.get('support', np.nan)),\n",
    "            \"confidence\": float(row.get('confidence', np.nan)),\n",
    "            \"lift\": float(row.get('lift', np.nan)),\n",
    "            \"leverage\": float(row.get('leverage', np.nan)) if 'leverage' in row else None,\n",
    "            \"conviction\": float(row.get('conviction', np.nan)) if 'conviction' in row else None\n",
    "        })\n",
    "    return serial\n",
    "\n",
    "def make_recommender_from_rules_list(rules_list: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Top-level factory to create a recommend() function from serializable rules.\n",
    "    Returns (recommend_function, internal_rules)\n",
    "    \"\"\"\n",
    "    # Convert antecedents to sets for fast subset checks\n",
    "    rules_internal = []\n",
    "    for r in rules_list:\n",
    "        rules_internal.append({\n",
    "            'antecedents': set(r.get('antecedents', [])),\n",
    "            'consequents': list(r.get('consequents', [])),\n",
    "            'support': r.get('support', 0.0),\n",
    "            'confidence': r.get('confidence', 0.0),\n",
    "            'lift': r.get('lift', 0.0)\n",
    "        })\n",
    "\n",
    "    def recommend(given_items, top_n=5, metric='lift'):\n",
    "        if isinstance(given_items, str):\n",
    "            given = set([given_items])\n",
    "        else:\n",
    "            given = set(given_items)\n",
    "        scores = {}\n",
    "        for r in rules_internal:\n",
    "            if r['antecedents'].issubset(given):\n",
    "                for c in r['consequents']:\n",
    "                    scores[c] = max(scores.get(c, 0.0), r.get(metric, 0.0))\n",
    "        for g in list(given):\n",
    "            scores.pop(g, None)\n",
    "        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [item for item, sc in sorted_items[:top_n]]\n",
    "\n",
    "    return recommend, rules_internal\n",
    "\n",
    "# ------------------ Main flow ------------------\n",
    "\n",
    "def main():\n",
    "    out_dir = ensure_output_dir(OUTPUT_DIR)\n",
    "    print(\"Loading dataset...\")\n",
    "    df = load_dataset(INPUT_CSV)\n",
    "    print(f\"Dataset loaded. Shape: {df.shape}. Columns: {list(df.columns)}\")\n",
    "\n",
    "    tx_col, item_col = guess_columns(df)\n",
    "    print(f\"Guessed transaction column: {tx_col}, item column: {item_col}\")\n",
    "\n",
    "    if tx_col is None or item_col is None:\n",
    "        raise ValueError(\"Could not determine transaction and/or item columns. Please inspect dataset and adjust script.\")\n",
    "\n",
    "    print(\"Preprocessing transactions and building basket matrix...\")\n",
    "    transactions, basket_df, mlb = preprocess_transactions(df, tx_col, item_col)\n",
    "    print(f\"Built basket matrix. Transactions: {len(transactions)}, Unique items: {basket_df.shape[1]}\")\n",
    "\n",
    "    # Save processed data (h5 via h5py or npz fallback)\n",
    "    processed_base = out_dir / \"processed_data\"\n",
    "    processed_path = save_processed_data_array(basket_df, processed_base)\n",
    "\n",
    "    # Run mining with fallback\n",
    "    frequent_itemsets, rules_df, used_support = run_mining_with_fallback(\n",
    "        basket_df,\n",
    "        min_support=INITIAL_MIN_SUPPORT,\n",
    "        min_confidence=MIN_CONFIDENCE,\n",
    "        max_len=MAX_ITEMSET_LEN\n",
    "    )\n",
    "    print(f\"Mining finished (used_min_support={used_support:.4f}). Found {len(frequent_itemsets)} frequent_itemsets and {len(rules_df)} rules.\")\n",
    "\n",
    "    # Convert rules to serializable form (store this in artifacts)\n",
    "    rules_serial = rules_to_serializable(rules_df)\n",
    "\n",
    "    # Save artifacts (do NOT include nested functions)\n",
    "    artifacts = {\n",
    "        \"mlb\": mlb,\n",
    "        \"basket_df_index\": list(map(str, basket_df.index)),\n",
    "        \"basket_columns\": list(map(str, basket_df.columns)),\n",
    "        \"frequent_itemsets\": frequent_itemsets,  # DataFrame picklable\n",
    "        \"rules_df\": rules_df,                   # DataFrame picklable\n",
    "        \"rules_serial\": rules_serial,           # JSON-serializable rules\n",
    "        \"raw_dataframe_head\": df.head(200)\n",
    "    }\n",
    "    pkl_path = out_dir / \"artifacts.pkl\"\n",
    "    save_pickle(artifacts, str(pkl_path))\n",
    "\n",
    "    # Save metadata YAML\n",
    "    metadata = {\n",
    "        \"source_csv\": INPUT_CSV,\n",
    "        \"generated_on\": datetime.datetime.now().isoformat(),\n",
    "        \"n_rows_raw\": int(df.shape[0]),\n",
    "        \"n_transactions\": int(basket_df.shape[0]),\n",
    "        \"n_unique_items\": int(basket_df.shape[1]),\n",
    "        \"tx_column\": tx_col,\n",
    "        \"item_column\": item_col,\n",
    "        \"min_support_requested\": INITIAL_MIN_SUPPORT,\n",
    "        \"min_support_used\": float(used_support),\n",
    "        \"min_confidence\": MIN_CONFIDENCE,\n",
    "        \"files\": {\n",
    "            \"processed\": str(processed_path),\n",
    "            \"pkl\": str(pkl_path),\n",
    "            \"rules_json\": str(out_dir / \"association_rules.json\"),\n",
    "            \"metadata_yaml\": str(out_dir / \"metadata.yaml\")\n",
    "        }\n",
    "    }\n",
    "    yaml_path = out_dir / \"metadata.yaml\"\n",
    "    save_yaml(metadata, str(yaml_path))\n",
    "\n",
    "    # Save association rules JSON (human-friendly)\n",
    "    json_path = out_dir / \"association_rules.json\"\n",
    "    save_json(rules_serial, str(json_path))\n",
    "\n",
    "    print(\"All artifacts generated successfully.\")\n",
    "    print(f\"Files written to: {out_dir}\")\n",
    "\n",
    "    if len(rules_serial) > 0:\n",
    "        print(\"\\nTop 10 rules (by lift):\")\n",
    "        for r in rules_serial[:10]:\n",
    "            print(f\"{r['antecedents']} -> {r['consequents']} (support={r['support']:.4f}, conf={r['confidence']:.4f}, lift={r['lift']:.4f})\")\n",
    "    else:\n",
    "        print(\"No rules generated with the chosen thresholds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ------------------ Example: how to load and use the recommender ------------------\n",
    "# After running the above script, in a separate Python session you can do:\n",
    "#\n",
    "# import joblib\n",
    "# from generate_artifacts_with_serial_rules import make_recommender_from_rules_list\n",
    "#\n",
    "# artifacts = joblib.load(r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\artifacts.pkl\")\n",
    "# rules_serial = artifacts.get('rules_serial', [])\n",
    "# recommend, internal_rules = make_recommender_from_rules_list(rules_serial)\n",
    "#\n",
    "# # Example usage:\n",
    "# print(recommend([\"whole milk\"], top_n=5, metric='lift'))\n",
    "# print(recommend([\"sausage\", \"brown bread\"], top_n=5))\n",
    "#\n",
    "# You can also log queries by writing to a CSV from the session that calls recommend().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3420741-8f83-47a2-8436-47f3af1d054c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
