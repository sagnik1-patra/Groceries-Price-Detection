{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a0adcc-4694-4492-b60d-2a0f564551b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data (processed_data). Transactions: 14963, Items: 167\n",
      "Hit-rates: {1: 0.0, 3: 0.0, 5: 0.0, 10: 0.0}\n",
      "Saved accuracy plot: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\accuracy_vs_k.png\n",
      "Saved heatmap: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\cooccurrence_heatmap_topN.png\n",
      "Saved evaluation_summary.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Optional: h5py to load processed_data.h5\n",
    "try:\n",
    "    import h5py\n",
    "    H5PY_AVAILABLE = True\n",
    "except Exception:\n",
    "    H5PY_AVAILABLE = False\n",
    "\n",
    "# ---------- USER PATHS (edit if different) ----------\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\")\n",
    "ARTIFACTS_PKL = OUTPUT_DIR / \"artifacts.pkl\"\n",
    "PROCESSED_H5 = OUTPUT_DIR / \"processed_data.h5\"   # may exist\n",
    "PROCESSED_NPZ = OUTPUT_DIR / \"processed_data.npz\" # fallback\n",
    "ORIGINAL_CSV = Path(r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\archive (1)\\Groceries_dataset.csv\")\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Evaluation config\n",
    "TOP_N_ITEMS_FOR_HEATMAP = 30     # heatmap will use top-N most frequent items\n",
    "EVAL_SAMPLE_LIMIT = 3000         # limit number of transactions to evaluate (for speed); set None to use all\n",
    "K_VALUES = [1, 3, 5, 10]         # compute hit-rate@k for these k values\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ------------------ Utilities: load artifacts & processed data ------------------\n",
    "\n",
    "def load_artifacts(pkl_path: Path) -> Dict[str, Any]:\n",
    "    if not pkl_path.exists():\n",
    "        raise FileNotFoundError(f\"Artifacts file not found: {pkl_path}\")\n",
    "    artifacts = joblib.load(str(pkl_path))\n",
    "    return artifacts\n",
    "\n",
    "def load_processed_array(h5_path: Path = None, npz_path: Path = None) -> Tuple[np.ndarray, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Returns (data_np, index_list, columns_list)\n",
    "    data_np shape: (n_transactions, n_items) dtype uint8 (0/1)\n",
    "    index_list: list of transaction ids (strings)\n",
    "    columns_list: list of item names (strings)\n",
    "    \"\"\"\n",
    "    if h5_path and h5_path.exists() and H5PY_AVAILABLE:\n",
    "        with h5py.File(str(h5_path), 'r') as hf:\n",
    "            data = np.array(hf['data']).astype(np.uint8)\n",
    "            try:\n",
    "                index = [x.decode('utf-8') if isinstance(x, (bytes, np.bytes_)) else str(x) for x in hf['index'][()]]\n",
    "            except Exception:\n",
    "                index = [str(x) for x in hf['index'][()]]\n",
    "            try:\n",
    "                columns = [x.decode('utf-8') if isinstance(x, (bytes, np.bytes_)) else str(x) for x in hf['columns'][()]]\n",
    "            except Exception:\n",
    "                columns = [str(x) for x in hf['columns'][()]]\n",
    "        return data, index, columns\n",
    "\n",
    "    if npz_path and npz_path.exists():\n",
    "        npz = np.load(str(npz_path), allow_pickle=True)\n",
    "        data = npz['data'].astype(np.uint8)\n",
    "        index = [str(x) for x in npz['index']]\n",
    "        columns = [str(x) for x in npz['columns']]\n",
    "        return data, index, columns\n",
    "\n",
    "    raise FileNotFoundError(\"No processed_data.h5 or processed_data.npz found.\")\n",
    "\n",
    "\n",
    "def rebuild_basket_from_csv(csv_path: Path, tx_col_guess: str = None, item_col_guess: str = None):\n",
    "    \"\"\"\n",
    "    Fallback: rebuild basket_df from original CSV. Uses same heuristics as earlier scripts.\n",
    "    Returns (data_np, index_list, columns_list).\n",
    "    \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "    df = pd.read_csv(str(csv_path), encoding='latin-1', low_memory=False)\n",
    "    # Heuristic: find columns\n",
    "    object_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    tx_col = tx_col_guess or next((c for c in object_cols if c.lower().find('date') != -1 or c.lower().find('invoice') != -1 or c.lower().find('order') != -1), None)\n",
    "    item_col = item_col_guess or next((c for c in object_cols if 'item' in c.lower() or 'description' in c.lower() or 'product' in c.lower()), None)\n",
    "    if tx_col is None or item_col is None:\n",
    "        # fallback to columns positions\n",
    "        tx_col = tx_col or df.columns[0]\n",
    "        item_col = item_col or df.columns[-1]\n",
    "    # If Date & Member_number present, group by (Member_number, Date)\n",
    "    if tx_col.lower() == 'date' and 'Member_number' in df.columns:\n",
    "        grouped = df.groupby(['Member_number', 'Date'])[item_col].apply(lambda s: list(pd.Series(s).astype(str).str.strip().unique()))\n",
    "    else:\n",
    "        grouped = df.groupby(tx_col)[item_col].apply(lambda s: list(pd.Series(s).astype(str).str.strip().unique()))\n",
    "    transactions = grouped.tolist()\n",
    "    tx_index = grouped.index.tolist()\n",
    "    # build one-hot\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    basket = mlb.fit_transform(transactions)\n",
    "    data = basket.astype(np.uint8)\n",
    "    columns = list(mlb.classes_)\n",
    "    index = [str(x) for x in tx_index]\n",
    "    return data, index, columns\n",
    "\n",
    "\n",
    "# ------------------ Recommender factory (same as in artifact script) ------------------\n",
    "\n",
    "def make_recommender_from_rules_list(rules_list):\n",
    "    \"\"\"\n",
    "    rules_list: list of dicts with keys 'antecedents','consequents','support','confidence','lift',...\n",
    "    Returns recommend(given_items, top_n=5, metric='lift')\n",
    "    \"\"\"\n",
    "    rules_internal = []\n",
    "    for r in rules_list:\n",
    "        rules_internal.append({\n",
    "            'antecedents': set(r.get('antecedents', [])),\n",
    "            'consequents': list(r.get('consequents', [])),\n",
    "            'support': r.get('support', 0.0),\n",
    "            'confidence': r.get('confidence', 0.0),\n",
    "            'lift': r.get('lift', 0.0)\n",
    "        })\n",
    "\n",
    "    def recommend(given_items, top_n=5, metric='lift'):\n",
    "        if isinstance(given_items, str):\n",
    "            given = set([given_items])\n",
    "        else:\n",
    "            given = set(given_items)\n",
    "        scores = {}\n",
    "        for r in rules_internal:\n",
    "            if r['antecedents'].issubset(given):\n",
    "                for c in r['consequents']:\n",
    "                    scores[c] = max(scores.get(c, 0.0), r.get(metric, 0.0))\n",
    "        for g in list(given):\n",
    "            scores.pop(g, None)\n",
    "        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [item for item, sc in sorted_items[:top_n]]\n",
    "\n",
    "    return recommend\n",
    "\n",
    "\n",
    "# ------------------ Evaluation: leave-one-out hit-rate@k ------------------\n",
    "\n",
    "def prepare_transactions_list(data_np: np.ndarray, index: List[str], columns: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Convert dense matrix to list of items per transaction.\n",
    "    data_np: shape (n_tx, n_items)\n",
    "    \"\"\"\n",
    "    tx_list = []\n",
    "    for i in range(data_np.shape[0]):\n",
    "        present_idx = np.where(data_np[i] > 0)[0]\n",
    "        items = [columns[j] for j in present_idx]\n",
    "        tx_list.append(items)\n",
    "    return tx_list\n",
    "\n",
    "\n",
    "def evaluate_hit_rate(rules_serial: List[Dict[str, Any]],\n",
    "                      transactions_list: List[List[str]],\n",
    "                      ks: List[int] = [1, 3, 5, 10],\n",
    "                      sample_limit: int = None) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Leave-one-out: for each transaction with >=2 items, hide one item and ask recommender to predict it.\n",
    "    Computes hit-rate@k = fraction of cases where hidden item is in top-k recommendations.\n",
    "    \"\"\"\n",
    "    # Build recommender\n",
    "    recommender = make_recommender_from_rules_list(rules_serial)\n",
    "\n",
    "    # prepare candidate transactions: those with length >= 2\n",
    "    valid_tx_idx = [i for i, t in enumerate(transactions_list) if len(t) >= 2]\n",
    "    if sample_limit is not None and len(valid_tx_idx) > sample_limit:\n",
    "        valid_tx_idx = list(np.random.choice(valid_tx_idx, size=sample_limit, replace=False))\n",
    "\n",
    "    hits = {k: 0 for k in ks}\n",
    "    total = 0\n",
    "\n",
    "    for idx in valid_tx_idx:\n",
    "        t = transactions_list[idx]\n",
    "        # random holdout one item\n",
    "        hold = random.choice(t)\n",
    "        given = [x for x in t if x != hold]\n",
    "        if len(given) == 0:\n",
    "            continue\n",
    "        total += 1\n",
    "        # get recommendations\n",
    "        for k in ks:\n",
    "            recs = recommender(given, top_n=k, metric='lift')\n",
    "            if hold in recs:\n",
    "                hits[k] += 1\n",
    "\n",
    "    if total == 0:\n",
    "        return {k: 0.0 for k in ks}\n",
    "\n",
    "    hit_rates = {k: hits[k] / total for k in ks}\n",
    "    return hit_rates\n",
    "\n",
    "\n",
    "# ------------------ Heatmap: co-occurrence matrix ------------------\n",
    "\n",
    "def compute_cooccurrence_matrix(data_np: np.ndarray, columns: List[str], top_n: int = 30, normalize: bool = False):\n",
    "    \"\"\"\n",
    "    data_np: (n_tx, n_items) 0/1\n",
    "    Returns (df_cooc, top_columns)\n",
    "    df_cooc: pandas DataFrame top_n x top_n with co-occurrence counts (or normalized support)\n",
    "    \"\"\"\n",
    "    # item frequencies\n",
    "    item_counts = data_np.sum(axis=0)\n",
    "    top_idx = np.argsort(-item_counts)[:top_n]\n",
    "    top_cols = [columns[i] for i in top_idx]\n",
    "    sub = data_np[:, top_idx].astype(np.uint8)\n",
    "\n",
    "    # co-occurrence counts: item_i with item_j across transactions\n",
    "    # compute via matrix multiplication\n",
    "    cooc = np.dot(sub.T, sub)  # shape (top_n, top_n)\n",
    "    df_cooc = pd.DataFrame(cooc, index=top_cols, columns=top_cols)\n",
    "\n",
    "    if normalize:\n",
    "        n_tx = data_np.shape[0]\n",
    "        df_cooc = df_cooc / n_tx  # convert to support (0..1)\n",
    "    return df_cooc, top_cols\n",
    "\n",
    "\n",
    "# ------------------ Plotting ------------------\n",
    "\n",
    "def plot_accuracy(hit_rates: Dict[int, float], out_path: Path):\n",
    "    ks = sorted(hit_rates.keys())\n",
    "    vals = [hit_rates[k] for k in ks]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(ks, vals, marker='o')\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel(\"k (top-k recommendations)\")\n",
    "    plt.ylabel(\"Hit-rate (Recall@k)\")\n",
    "    plt.title(\"Recommender Hit-rate vs k (leave-one-out)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(out_path), dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved accuracy plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_heatmap(df_cooc: pd.DataFrame, out_path: Path, annot: bool = False, cmap: str = \"YlGnBu\"):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # use logarithmic scaling when values vary a lot (optional)\n",
    "    sns.heatmap(df_cooc, cmap=cmap, annot=annot, fmt=\".2f\" if df_cooc.values.dtype.kind == 'f' else \"d\")\n",
    "    plt.title(\"Item Co-occurrence Heatmap (top items)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(out_path), dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved heatmap: {out_path}\")\n",
    "\n",
    "\n",
    "# ------------------ Main runner ------------------\n",
    "\n",
    "def main():\n",
    "    # 1) load artifacts (rules), and processed matrix (or rebuild)\n",
    "    try:\n",
    "        artifacts = load_artifacts(ARTIFACTS_PKL)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not load artifacts.pkl: {e}\")\n",
    "        artifacts = {}\n",
    "\n",
    "    # load rules_serial\n",
    "    rules_serial = artifacts.get('rules_serial', None)\n",
    "\n",
    "    # load processed data\n",
    "    data_np = None\n",
    "    columns = None\n",
    "    index = None\n",
    "    loaded_from = None\n",
    "\n",
    "    try:\n",
    "        data_np, index, columns = load_processed_array(PROCESSED_H5 if PROCESSED_H5.exists() else None,\n",
    "                                                       PROCESSED_NPZ if PROCESSED_NPZ.exists() else None)\n",
    "        loaded_from = \"processed_data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load processed_data file: {e}. Will attempt to rebuild from CSV.\")\n",
    "        try:\n",
    "            data_np, index, columns = rebuild_basket_from_csv(ORIGINAL_CSV)\n",
    "            loaded_from = \"rebuilt_from_csv\"\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"Failed to load or rebuild processed data: {e2}\")\n",
    "\n",
    "    print(f\"Loaded processed data ({loaded_from}). Transactions: {data_np.shape[0]}, Items: {data_np.shape[1]}\")\n",
    "\n",
    "    # If rules not present, try loading rules_json file\n",
    "    if rules_serial is None:\n",
    "        rules_json_path = OUTPUT_DIR / \"association_rules.json\"\n",
    "        if rules_json_path.exists():\n",
    "            with open(str(rules_json_path), 'r', encoding='utf-8') as fh:\n",
    "                rules_serial = json.load(fh)\n",
    "            print(\"Loaded rules from association_rules.json\")\n",
    "        else:\n",
    "            print(\"No rules found in artifacts or association_rules.json — evaluation will produce zero metrics.\")\n",
    "            rules_serial = []\n",
    "\n",
    "    # Build transactions list\n",
    "    transactions_list = prepare_transactions_list(data_np, index, columns)\n",
    "\n",
    "    # 2) Evaluate hit-rate@k\n",
    "    hit_rates = evaluate_hit_rate(rules_serial, transactions_list, ks=K_VALUES, sample_limit=EVAL_SAMPLE_LIMIT)\n",
    "    print(\"Hit-rates:\", hit_rates)\n",
    "\n",
    "    # Plot accuracy graph\n",
    "    acc_out = OUTPUT_DIR / \"accuracy_vs_k.png\"\n",
    "    plot_accuracy(hit_rates, acc_out)\n",
    "\n",
    "    # 3) Compute co-occurrence heatmap (top N)\n",
    "    df_cooc, top_cols = compute_cooccurrence_matrix(data_np, columns, top_n=TOP_N_ITEMS_FOR_HEATMAP, normalize=False)\n",
    "    heatmap_out = OUTPUT_DIR / \"cooccurrence_heatmap_topN.png\"\n",
    "    plot_heatmap(df_cooc, heatmap_out, annot=False)\n",
    "\n",
    "    # Save a small CSV summary\n",
    "    summary = {\n",
    "        \"hit_rates\": hit_rates,\n",
    "        \"n_transactions\": int(data_np.shape[0]),\n",
    "        \"n_items\": int(data_np.shape[1]),\n",
    "        \"top_items_for_heatmap\": top_cols\n",
    "    }\n",
    "    with open(str(OUTPUT_DIR / \"evaluation_summary.json\"), 'w', encoding='utf-8') as fh:\n",
    "        json.dump(summary, fh, indent=2)\n",
    "    print(\"Saved evaluation_summary.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203521a-046e-4b31-86fe-8c5824f39e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
