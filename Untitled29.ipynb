{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115886db-96df-412f-bfdf-ef9716c4efa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.16.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.7.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (3.10.6)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
      "Downloading mlxtend-0.23.4-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 1.6 MB/s  0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.4\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6e67dc-0bbf-4554-96ef-f2d83d998000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tables\n",
      "  Downloading tables-3.10.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (1.26.4)\n",
      "Collecting numexpr>=2.6.2 (from tables)\n",
      "  Downloading numexpr-2.12.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (25.0)\n",
      "Collecting py-cpuinfo (from tables)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting blosc2>=2.3.0 (from tables)\n",
      "  Downloading blosc2-3.8.0-cp311-cp311-win_amd64.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (4.15.0)\n",
      "Collecting ndindex (from blosc2>=2.3.0->tables)\n",
      "  Downloading ndindex-1.10.0-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting msgpack (from blosc2>=2.3.0->tables)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from blosc2>=2.3.0->tables) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from blosc2>=2.3.0->tables) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (2025.8.3)\n",
      "Downloading tables-3.10.2-cp311-cp311-win_amd64.whl (6.4 MB)\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.4 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.4 MB 1.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.3/6.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.1/6.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.7/6.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.4 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.5/6.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.0/6.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.2/6.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.4/6.4 MB 1.9 MB/s  0:00:03\n",
      "Downloading blosc2-3.8.0-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.3 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.9 MB/s  0:00:01\n",
      "Downloading numexpr-2.12.1-cp311-cp311-win_amd64.whl (151 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-win_amd64.whl (72 kB)\n",
      "Downloading ndindex-1.10.0-cp311-cp311-win_amd64.whl (156 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, numexpr, ndindex, msgpack, blosc2, tables\n",
      "\n",
      "   ------ --------------------------------- 1/6 [numexpr]\n",
      "   ------------- -------------------------- 2/6 [ndindex]\n",
      "   -------------------------- ------------- 4/6 [blosc2]\n",
      "   -------------------------- ------------- 4/6 [blosc2]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   --------------------------------- ------ 5/6 [tables]\n",
      "   ---------------------------------------- 6/6 [tables]\n",
      "\n",
      "Successfully installed blosc2-3.8.0 msgpack-1.1.1 ndindex-1.10.0 numexpr-2.12.1 py-cpuinfo-9.0.0 tables-3.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d65fd0e-fe2f-4c6d-bba4-9ed4e17001f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded. Shape: (38765, 3). Columns: ['Member_number', 'Date', 'itemDescription']\n",
      "Guessed transaction column: Date, item column: itemDescription\n",
      "Preprocessing transactions and building basket matrix...\n",
      "Built basket matrix. Transactions: 728, Unique items: 167\n",
      "Saved HDF5: C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\processed_data.h5\n",
      "Running Apriori frequent itemset mining and association rules...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 126. GiB for an array with shape (37169919, 5, 728) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 257\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo rules generated with the chosen support/confidence thresholds. Consider lowering min_support/min_confidence in script.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Run Apriori\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning Apriori frequent itemset mining and association rules...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m frequent_itemsets, rules = \u001b[43mrun_apriori_and_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasket_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(frequent_itemsets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m frequent itemsets and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rules.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Build recommender object\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mrun_apriori_and_rules\u001b[39m\u001b[34m(basket_df, min_support, min_confidence)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# apriori expects booleans or 0/1; ensure boolean\u001b[39;00m\n\u001b[32m     96\u001b[39m bool_df = basket_df.astype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m frequent_itemsets = \u001b[43mapriori\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbool_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_colnames\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# add length of itemset\u001b[39;00m\n\u001b[32m     99\u001b[39m frequent_itemsets[\u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m] = frequent_itemsets[\u001b[33m'\u001b[39m\u001b[33mitemsets\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlxtend\\frequent_patterns\\apriori.py:309\u001b[39m, in \u001b[36mapriori\u001b[39m\u001b[34m(df, min_support, use_colnames, max_len, verbose, low_memory)\u001b[39m\n\u001b[32m    307\u001b[39m         _bools = _bools & (X[:, combin[:, n]] == all_ones)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     _bools = np.all(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombin\u001b[49m\u001b[43m]\u001b[49m, axis=\u001b[32m2\u001b[39m)\n\u001b[32m    311\u001b[39m support = _support(np.array(_bools), rows_count, is_sparse)\n\u001b[32m    312\u001b[39m _mask = (support >= min_support).reshape(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 126. GiB for an array with shape (37169919, 5, 728) and data type bool"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "# ---------- USER PATHS (as requested) ----------\n",
    "INPUT_CSV = r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\\archive (1)\\Groceries_dataset.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Groceries Price Detection\"\n",
    "# ------------------------------------------------\n",
    "\n",
    "def ensure_output_dir(path):\n",
    "    p = Path(path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def guess_columns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Try to guess which columns are transaction id and item description columns.\n",
    "    Returns (transaction_col, item_col). If uncertain, falls back to first two object columns.\n",
    "    \"\"\"\n",
    "    object_cols = [c for c, dt in df.dtypes.items() if pd.api.types.is_object_dtype(dt) or pd.api.types.is_string_dtype(dt)]\n",
    "    lower = [c.lower() for c in object_cols]\n",
    "\n",
    "    # common names\n",
    "    tx_candidates = []\n",
    "    item_candidates = []\n",
    "\n",
    "    tx_names = [\"invoice\", \"invoiceNo\", \"invoiceno\", \"transaction\", \"orderid\", \"order_id\", \"basket\", \"receipt\", \"id\"]\n",
    "    item_names = [\"item\", \"itemDescription\", \"product\", \"productDescription\", \"item_description\", \"itemdesc\", \"item_name\", \"description\"]\n",
    "\n",
    "    for col in object_cols:\n",
    "        cl = col.lower()\n",
    "        if any(name.lower() in cl for name in tx_names):\n",
    "            tx_candidates.append(col)\n",
    "        if any(name.lower() in cl for name in item_names):\n",
    "            item_candidates.append(col)\n",
    "\n",
    "    # If none found, fallback to first two object columns\n",
    "    tx_col = tx_candidates[0] if tx_candidates else (object_cols[0] if len(object_cols) >= 1 else None)\n",
    "    item_col = item_candidates[0] if item_candidates else (object_cols[1] if len(object_cols) >= 2 else (object_cols[0] if object_cols else None))\n",
    "\n",
    "    return tx_col, item_col\n",
    "\n",
    "def load_dataset(csv_path: str):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Input CSV not found: {csv_path}\")\n",
    "    # try reading with default, if fails try latin-1\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "    return df\n",
    "\n",
    "def preprocess_transactions(df: pd.DataFrame, tx_col: str, item_col: str):\n",
    "    \"\"\"\n",
    "    Build transactions list and a one-hot encoded basket DataFrame.\n",
    "    Returns:\n",
    "      - transactions: list of sets/items per transaction\n",
    "      - basket_df: one-hot encoded DataFrame, rows=transactions, cols=items (0/1)\n",
    "      - tx_index: list of transaction ids (index aligned with basket_df)\n",
    "    \"\"\"\n",
    "    # drop rows with missing item\n",
    "    df = df.copy()\n",
    "    df[item_col] = df[item_col].astype(str).str.strip()\n",
    "    df = df[df[item_col].notna() & (df[item_col] != '')]\n",
    "    # If tx_col is missing, treat each row as a transaction\n",
    "    if tx_col is None:\n",
    "        df['_TX_AUTO_'] = range(len(df))\n",
    "        tx_col = '_TX_AUTO_'\n",
    "    # Group by transaction id -> list of unique items\n",
    "    grouped = df.groupby(tx_col)[item_col].apply(lambda s: list(pd.Series(s).astype(str).str.strip().unique()))\n",
    "    transactions = grouped.tolist()\n",
    "    tx_index = grouped.index.tolist()\n",
    "\n",
    "    # MultiLabelBinarizer to one-hot\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    basket = mlb.fit_transform(transactions)\n",
    "    basket_df = pd.DataFrame(basket, index=tx_index, columns=mlb.classes_)\n",
    "    # Convert to 0/1 int\n",
    "    basket_df = basket_df.astype(int)\n",
    "    return transactions, basket_df, mlb\n",
    "\n",
    "def run_apriori_and_rules(basket_df: pd.DataFrame, min_support=0.01, min_confidence=0.2):\n",
    "    \"\"\"\n",
    "    Run Apriori and derive association rules.\n",
    "    Returns frequent_itemsets (DataFrame) and rules (DataFrame).\n",
    "    \"\"\"\n",
    "    # apriori expects booleans or 0/1; ensure boolean\n",
    "    bool_df = basket_df.astype(bool)\n",
    "    frequent_itemsets = apriori(bool_df, min_support=min_support, use_colnames=True)\n",
    "    # add length of itemset\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    # generate rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    # sort rules\n",
    "    if not rules.empty:\n",
    "        rules = rules.sort_values(['lift', 'confidence', 'support'], ascending=[False, False, False])\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "def save_hdf5(df: pd.DataFrame, out_path: str, key='data'):\n",
    "    # pandas HDF5\n",
    "    df.to_hdf(out_path, key=key, mode='w', format='table')\n",
    "    print(f\"Saved HDF5: {out_path}\")\n",
    "\n",
    "def save_pickle(obj, out_path: str):\n",
    "    joblib.dump(obj, out_path)\n",
    "    print(f\"Saved PKL: {out_path}\")\n",
    "\n",
    "def save_yaml(data: dict, out_path: str):\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.safe_dump(data, f, sort_keys=False)\n",
    "    print(f\"Saved YAML: {out_path}\")\n",
    "\n",
    "def save_json(data, out_path: str):\n",
    "    # data should be JSON serializable\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, default=str)\n",
    "    print(f\"Saved JSON: {out_path}\")\n",
    "\n",
    "def rules_to_serializable(rules_df: pd.DataFrame):\n",
    "    \"\"\"Convert rules DataFrame into a JSON-serializable list of dicts (top fields).\"\"\"\n",
    "    if rules_df.empty:\n",
    "        return []\n",
    "    serial = []\n",
    "    for _, row in rules_df.iterrows():\n",
    "        serial.append({\n",
    "            \"antecedents\": sorted(list(row['antecedents'])) if hasattr(row['antecedents'], '__iter__') else [row['antecedents']],\n",
    "            \"consequents\": sorted(list(row['consequents'])) if hasattr(row['consequents'], '__iter__') else [row['consequents']],\n",
    "            \"support\": float(row.get('support', np.nan)),\n",
    "            \"confidence\": float(row.get('confidence', np.nan)),\n",
    "            \"lift\": float(row.get('lift', np.nan)),\n",
    "            \"leverage\": float(row.get('leverage', np.nan)) if 'leverage' in row else None,\n",
    "            \"conviction\": float(row.get('conviction', np.nan)) if 'conviction' in row else None\n",
    "        })\n",
    "    return serial\n",
    "\n",
    "def build_recommender(rules_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build a simple item-based recommender function using rules DataFrame.\n",
    "    The returned object is a dict with a function 'recommend' (callable) and stored rules.\n",
    "    recommend(items, top_n=5, metric='lift') -> list of recommended items\n",
    "    \"\"\"\n",
    "    # Simplified rules aggregation: for each antecedent set, map to consequents with scores\n",
    "    rules_list = rules_to_serializable(rules_df)\n",
    "\n",
    "    def recommend(given_items, top_n=5, metric='lift'):\n",
    "        if isinstance(given_items, str):\n",
    "            given = set([given_items])\n",
    "        else:\n",
    "            given = set(given_items)\n",
    "        scores = {}\n",
    "        for r in rules_list:\n",
    "            ant = set(r['antecedents'])\n",
    "            if ant.issubset(given):\n",
    "                for c in r['consequents']:\n",
    "                    # score by chosen metric (lift or confidence)\n",
    "                    scores[c] = max(scores.get(c, 0), r.get(metric, 0))\n",
    "        # remove items already in given\n",
    "        for g in list(given):\n",
    "            scores.pop(g, None)\n",
    "        # return top_n sorted by score\n",
    "        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [item for item, sc in sorted_items[:top_n]]\n",
    "\n",
    "    recommender_obj = {\n",
    "        \"rules\": rules_list,\n",
    "        \"recommend\": recommend  # note: function not JSON serializable but will be pickled\n",
    "    }\n",
    "    return recommender_obj\n",
    "\n",
    "def main():\n",
    "    out_dir = ensure_output_dir(OUTPUT_DIR)\n",
    "    print(\"Loading dataset...\")\n",
    "    df = load_dataset(INPUT_CSV)\n",
    "    print(f\"Dataset loaded. Shape: {df.shape}. Columns: {list(df.columns)}\")\n",
    "\n",
    "    tx_col, item_col = guess_columns(df)\n",
    "    print(f\"Guessed transaction column: {tx_col}, item column: {item_col}\")\n",
    "\n",
    "    if tx_col is None or item_col is None:\n",
    "        raise ValueError(\"Could not determine transaction and/or item columns. Please inspect dataset and adjust script.\")\n",
    "\n",
    "    print(\"Preprocessing transactions and building basket matrix...\")\n",
    "    transactions, basket_df, mlb = preprocess_transactions(df, tx_col, item_col)\n",
    "    print(f\"Built basket matrix. Transactions: {len(transactions)}, Unique items: {basket_df.shape[1]}\")\n",
    "\n",
    "    # Save HDF5 of the processed basket (it can be large; consider compression if needed)\n",
    "    h5_path = out_dir / \"processed_data.h5\"\n",
    "    save_hdf5(basket_df, str(h5_path), key='basket')\n",
    "\n",
    "    # Run Apriori\n",
    "    print(\"Running Apriori frequent itemset mining and association rules...\")\n",
    "    frequent_itemsets, rules = run_apriori_and_rules(basket_df, min_support=0.01, min_confidence=0.2)\n",
    "    print(f\"Found {len(frequent_itemsets)} frequent itemsets and {len(rules)} rules.\")\n",
    "\n",
    "    # Build recommender object\n",
    "    recommender = build_recommender(rules)\n",
    "\n",
    "    # Save PKL: include useful objects\n",
    "    artifacts = {\n",
    "        \"mlb\": mlb,\n",
    "        \"basket_df_index\": list(basket_df.index),\n",
    "        \"basket_columns\": list(basket_df.columns),\n",
    "        \"frequent_itemsets\": frequent_itemsets,\n",
    "        \"rules\": rules,\n",
    "        \"recommender\": recommender,\n",
    "        \"raw_dataframe_head\": df.head(100)  # small snapshot\n",
    "    }\n",
    "    pkl_path = out_dir / \"artifacts.pkl\"\n",
    "    save_pickle(artifacts, str(pkl_path))\n",
    "\n",
    "    # Save YAML metadata/config\n",
    "    metadata = {\n",
    "        \"source_csv\": INPUT_CSV,\n",
    "        \"generated_on\": datetime.datetime.now().isoformat(),\n",
    "        \"n_rows_raw\": int(df.shape[0]),\n",
    "        \"n_transactions\": int(basket_df.shape[0]),\n",
    "        \"n_unique_items\": int(basket_df.shape[1]),\n",
    "        \"tx_column\": tx_col,\n",
    "        \"item_column\": item_col,\n",
    "        \"apriori_min_support\": 0.01,\n",
    "        \"apriori_min_confidence\": 0.2,\n",
    "        \"files\": {\n",
    "            \"hdf5\": str(h5_path),\n",
    "            \"pkl\": str(pkl_path),\n",
    "            \"rules_json\": str(out_dir / \"association_rules.json\"),\n",
    "            \"metadata_yaml\": str(out_dir / \"metadata.yaml\")\n",
    "        }\n",
    "    }\n",
    "    yaml_path = out_dir / \"metadata.yaml\"\n",
    "    save_yaml(metadata, str(yaml_path))\n",
    "\n",
    "    # Save association rules to JSON\n",
    "    rules_json = rules_to_serializable(rules)\n",
    "    json_path = out_dir / \"association_rules.json\"\n",
    "    save_json(rules_json, str(json_path))\n",
    "\n",
    "    print(\"All artifacts generated successfully.\")\n",
    "    print(f\"Files written to: {out_dir}\")\n",
    "\n",
    "    # print a tiny sample of top rules\n",
    "    if len(rules_json) > 0:\n",
    "        print(\"\\nTop 10 rules (by lift):\")\n",
    "        for r in rules_json[:10]:\n",
    "            print(f\"{r['antecedents']} -> {r['consequents']} (support={r['support']:.4f}, conf={r['confidence']:.4f}, lift={r['lift']:.4f})\")\n",
    "    else:\n",
    "        print(\"No rules generated with the chosen support/confidence thresholds. Consider lowering min_support/min_confidence in script.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b6d5a-8c75-40ac-aac1-2832a479fd17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
